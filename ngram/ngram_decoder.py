#!/usr/bin/env python

"""
Version 0.21

Generate/re-order with an n-gram language model

An arpa formatted language model (as for example, a Kneser-Ney LM generated by
KenLM) is used to re-order a shuffled sentence. Optionally, a unigram LM can
be used for future costs. 

BNPs, if used, should be marked with:
    a start symbol: <sonp>
    an end symbol:  <eonp>

In the case of base NP symbols (BNPs) in the input, the n-gram models should be 
trained with BNPs. By default base NP symbols are treated as words. To
disable this, use the --no_npsyms_as_words flag, in which case the LMs should
be trained without BNP symbols, but the input can still contain BNPs, which
will still be correctly handled/scored in the beam decoder (but the BNP symbols
themselves will be ignored).

In the case of no base NPs in the input, the n-gram models should be trained
without base NP symbols. 

The KenLM Python bindings are required.

Neither the input nor the LM training files should contain explicit EOS symbols.

The input file must be pre-shuffled. For consistency with past work in this
line of research, in the case of BNPs, shuffling should treat BNPs as atomic 
units. To replicate our results, low-frequency tokens should be replaced with
special tokens and the post-processing script should be used to randomly 
replace these from remaining unused words before calculating BLEU. 
See the repo README for additional details.

The re-ordered output is printed to standard out.

"""

import os
import sys
import argparse
import kenlm
import copy
from collections import namedtuple
import math

Hypothesis = namedtuple("Hypothesis", ['score', 'last_action', "bow", 
    "future_score", "state", "last_beam"])

def batch_advance(lm, inner_states, w, out_states):
    probs = []
    
    for state in inner_states:
        out_states.append(kenlm.State())
        probs.append(lm.BaseScore(state, w, out_states[-1]))
    
    return probs

def future(action, word_set, futurelm):
    """
    Pre-condition: action has been removed from word_set
    """
    score = 0
    if futurelm != {}:
        for words, c in word_set.iteritems():
            if c == 0: continue
            for i, w in enumerate(words):
                score += futurelm[w] * c

    return score

def generate(lm, bow, beam_size, futurelm):
    
    n = sum([v*len(action) for action, v in bow.iteritems()])
    start_state = kenlm.State()
    lm.BeginSentenceWrite(start_state)

    order = []
    beams = {}
    beams[0] = [Hypothesis(0, None, bow, future(None, bow, futurelm), 
        start_state, None)]
        
    for i in range(1, n+1):
        beams[i] = []

    for i in range(n):

        states = []
        
        for j, hyp in enumerate(beams[i]):
            states.append(hyp.state)
            
        for action in bow:
            # Advance
            inner_states = states
            scores = [0.0] * len(beams[i])
            
            for w in action:
                out_states = []
                new_scores = batch_advance(lm, inner_states, w, out_states)
                inner_states = out_states
                for new_score_i, new_score in enumerate(new_scores):
                    scores[new_score_i] += new_score

            # Add to beam.
            ni = i + len(action)
            
            for j, hyp in enumerate(beams[i]):
                score = scores[j]
                out_state = inner_states[j]
                
                if hyp.bow[action] == 0:
                    continue
                
                new_bow = copy.copy(hyp.bow)
                new_bow[action] -= 1
                fscore = future(action, new_bow, futurelm)               
                if len(beams[ni]) < beam_size or (hyp.score+score+fscore
                    > beams[ni][-1].score + beams[ni][-1].future_score):
 
                    new_hyp = Hypothesis(hyp.score+score, action, new_bow, 
                        fscore, out_state, j)
                    beams[ni].append(new_hyp)
                    beams[ni].sort(key=lambda a: a.score + a.future_score)
                    beams[ni].reverse()
                    beams[ni] = beams[ni][:beam_size]

    cur = n
    pos = 0
    while cur > 0:
        order.extend(reversed(beams[cur][pos].last_action))
        old_cur = cur
        cur -= len(beams[cur][pos].last_action)
        pos = beams[old_cur][pos].last_beam

    order.reverse()
    return order

def main(arguments):

    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawDescriptionHelpFormatter)

    parser.add_argument('lm', help="Language model", type=str)    
    parser.add_argument('test', help="Shuffled file (one sentence per line, no \
        EOS symbols, to re-order.", type=str)
    parser.add_argument('beamsize', help="Beam size to use.", type=int)
    parser.add_argument('-f', '--future', help="LM for unigram future costs. \
        If omitted, future costs are not calculated.", type=str, default="")
    
    parser.add_argument('-n', '--no_npsyms_as_words',
        help="Do not treat base NP symbols as words.", action="store_true")
        
    args = parser.parse_args(arguments)
    lm = kenlm.Model(args.lm)
    
    futurelm = {}
    if args.future != "":
        for l in open(args.future):
            t = l.strip().split()
            if len(t) == 2 and t[0] != "ngram":        
                futurelm[t[1]] = float(t[0])

    for l in open(args.test):

        bow = {}

        in_bnp = False
        cur_bnp = []
        for w in l.strip().split():
            if w == "<sonp>":
                in_bnp = True
                cur_bnp = []
                continue
            if w == "<eonp>":
                in_bnp = False
                if not args.no_npsyms_as_words:
                    cur_bnp = ["<sonp>"] + cur_bnp + ["<eonp>"]
                write = tuple(cur_bnp)
            else:
                write = (w,)

            if in_bnp:
                cur_bnp.append(w)
                continue

            bow.setdefault(write, 0)
            bow[write] += 1

        print " ".join(generate(lm, bow, args.beamsize, futurelm))


if __name__ == '__main__':
    sys.exit(main(sys.argv[1:]))
